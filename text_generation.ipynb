{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "path_to_file = \"./Data/shakespeare.txt\"\n",
    "text = open(path_to_file, \"r\").read()\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 ['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(len(vocab),vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding: Letters to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 84 unique characters in the shakespear corpus. We will use one-hot encoding to represent each character as a vector of length 84. The vector will have all zeros except for the index corresponding to the character, which will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}\n",
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind[\"H\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text])\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so there are around 5.5 Million words inside the shakespeare.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,\n",
       "        1, 59, 60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,\n",
       "        0,  1,  1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57,\n",
       "       60, 56, 76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64, 62, 63,\n",
       "       75,  1, 69, 60, 77, 60, 73,  1, 59, 64, 60,  8,  0,  1,  1, 27, 76,\n",
       "       75,  1, 56, 74,  1, 75, 63, 60,  1, 73, 64, 71, 60, 73,  1, 74, 63,\n",
       "       70, 76, 67, 59,  1, 57, 80,  1, 75, 64, 68, 60,  1, 59, 60, 58, 60,\n",
       "       56, 74, 60,  8,  0,  1,  1, 33, 64, 74,  1, 75, 60, 69, 59, 60, 73,\n",
       "        1, 63, 60, 64, 73,  1, 68, 64, 62, 63, 75,  1, 57, 60, 56, 73,  1,\n",
       "       63, 64, 74,  1, 68, 60, 68, 70, 73, 80, 21,  0,  1,  1, 27, 76, 75,\n",
       "        1, 75, 63, 70, 76,  1, 58, 70, 69, 75, 73, 56, 58, 75, 60, 59,  1,\n",
       "       75, 70,  1, 75, 63, 64, 69, 60,  1, 70, 78, 69,  1, 57, 73, 64, 62,\n",
       "       63, 75,  1, 60, 80, 60, 74,  8,  0,  1,  1, 31, 60, 60, 59,  5, 74,\n",
       "       75,  1, 75, 63, 80,  1, 67, 64, 62, 63, 75,  5, 74,  1, 61, 67, 56,\n",
       "       68, 60,  1, 78, 64, 75, 63,  1, 74, 60, 67, 61,  9, 74, 76, 57, 74,\n",
       "       75, 56, 69, 75, 64, 56, 67,  1, 61, 76, 60, 67,  8,  0,  1,  1, 38,\n",
       "       56, 66, 64, 69, 62,  1, 56,  1, 61, 56, 68, 64, 69, 60,  1, 78, 63,\n",
       "       60, 73, 60,  1, 56, 57, 76, 69, 59, 56, 69, 58, 60,  1, 67, 64, 60,\n",
       "       74,  8,  0,  1,  1, 45, 63, 80,  1, 74, 60, 67, 61,  1, 75, 63, 80,\n",
       "        1, 61, 70, 60,  8,  1, 75, 70,  1, 75, 63, 80,  1, 74, 78, 60, 60,\n",
       "       75,  1, 74, 60, 67, 61,  1, 75, 70, 70,  1, 58, 73, 76, 60, 67, 21,\n",
       "        0,  1,  1, 45, 63, 70, 76,  1, 75, 63, 56, 75,  1, 56, 73, 75,  1,\n",
       "       69, 70, 78,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  5, 74,  1, 61,\n",
       "       73, 60, 74, 63,  1, 70, 73, 69, 56, 68, 60, 69, 75,  8,  0,  1,  1,\n",
       "       26, 69, 59,  1, 70, 69, 67, 80,  1, 63, 60, 73, 56, 67, 59,  1, 75,\n",
       "       70,  1, 75, 63, 60,  1, 62, 56, 76, 59, 80,  1, 74, 71, 73, 64, 69,\n",
       "       62,  8,  0,  1,  1, 48, 64, 75, 63, 64, 69,  1, 75, 63, 64, 69, 60,\n",
       "        1, 70, 78, 69,  1, 57, 76])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"From fairest creatures we desire increase,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a rhyme in almost every 3rd line, so in order for our model to find patterns, it must read atleast 3 lines: around 130 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41569"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_seq = len(text) // (seq_length + 1)\n",
    "total_seq  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Tensors: Creating Batches\n",
    "SO we can easily feed them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "type(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "73\n",
      "70\n",
      "68\n",
      "1\n",
      "61\n",
      "56\n",
      "64\n",
      "73\n",
      "60\n",
      "74\n",
      "75\n",
      "1\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "75\n",
      "76\n",
      "73\n",
      "60\n",
      "74\n",
      "1\n",
      "78\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "74\n",
      "64\n",
      "73\n",
      "60\n",
      "1\n",
      "64\n",
      "69\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "57\n",
      "80\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "76\n",
      "75\n",
      "80\n",
      "5\n",
      "74\n",
      "1\n",
      "73\n",
      "70\n",
      "74\n",
      "60\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "69\n",
      "60\n",
      "77\n",
      "60\n",
      "73\n",
      "1\n",
      "59\n",
      "64\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "56\n",
      "74\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "73\n",
      "64\n",
      "71\n",
      "60\n",
      "73\n",
      "1\n",
      "74\n",
      "63\n",
      "70\n",
      "76\n",
      "67\n",
      "59\n",
      "1\n",
      "57\n",
      "80\n",
      "1\n",
      "75\n",
      "64\n",
      "68\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "58\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "33\n",
      "64\n",
      "74\n",
      "1\n",
      "75\n",
      "60\n",
      "69\n",
      "59\n",
      "60\n",
      "73\n",
      "1\n",
      "63\n",
      "60\n",
      "64\n",
      "73\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "73\n",
      "1\n",
      "63\n",
      "64\n",
      "74\n",
      "1\n",
      "68\n",
      "60\n",
      "68\n",
      "70\n",
      "73\n",
      "80\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "58\n",
      "70\n",
      "69\n",
      "75\n",
      "73\n",
      "56\n",
      "58\n",
      "75\n",
      "60\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "73\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "60\n",
      "80\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "60\n",
      "60\n",
      "59\n",
      "5\n",
      "74\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "67\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "67\n",
      "56\n",
      "68\n",
      "60\n",
      "1\n",
      "78\n",
      "64\n",
      "75\n",
      "63\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "9\n",
      "74\n",
      "76\n",
      "57\n",
      "74\n",
      "75\n",
      "56\n",
      "69\n",
      "75\n",
      "64\n",
      "56\n",
      "67\n",
      "1\n",
      "61\n",
      "76\n",
      "60\n",
      "67\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "38\n",
      "56\n",
      "66\n",
      "64\n",
      "69\n",
      "62\n",
      "1\n",
      "56\n",
      "1\n",
      "61\n",
      "56\n",
      "68\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "78\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "1\n",
      "56\n",
      "57\n",
      "76\n",
      "69\n",
      "59\n",
      "56\n",
      "69\n",
      "58\n",
      "60\n",
      "1\n",
      "67\n",
      "64\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "61\n",
      "70\n",
      "60\n",
      "8\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "78\n",
      "60\n",
      "60\n",
      "75\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "70\n",
      "70\n",
      "1\n",
      "58\n",
      "73\n",
      "76\n",
      "60\n",
      "67\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "75\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "56\n",
      "73\n",
      "75\n",
      "1\n",
      "69\n",
      "70\n",
      "78\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "78\n",
      "70\n",
      "73\n",
      "67\n",
      "59\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "73\n",
      "60\n",
      "74\n",
      "63\n",
      "1\n",
      "70\n",
      "73\n",
      "69\n",
      "56\n",
      "68\n",
      "60\n",
      "69\n",
      "75\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "26\n",
      "69\n",
      "59\n",
      "1\n",
      "70\n",
      "69\n",
      "67\n",
      "80\n",
      "1\n",
      "63\n",
      "60\n",
      "73\n",
      "56\n",
      "67\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "62\n",
      "56\n",
      "76\n",
      "59\n",
      "80\n",
      "1\n",
      "74\n",
      "71\n",
      "73\n",
      "64\n",
      "69\n",
      "62\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "48\n",
      "64\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "for char in char_dataset.take(500):\n",
    "    print(char.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sequences from the Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True) \n",
    "# Because of 0 indexing, seq_length + 1\n",
    "# len(text) / seq_length == 45005.03 so dropping that 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our sequences, we will perform the following steps for each one to create our target text sequences:\n",
    "\n",
    "1. Grab the input text sequence\n",
    "2. Assign the target text sequence as the input text sequence shifted by one step forward\n",
    "3. Group them together as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1] # Hello my nam\n",
    "    target_txt = seq[1:] # ello my name\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Input Sequence and Target Text\n",
    "\n",
    "Target has extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75\n",
      "  1 56 74  1 75 63 60  1 73 64]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the ri\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1\n",
      " 56 74  1 75 63 60  1 73 64 71]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the rip\n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training batches\n",
    "\n",
    "Now that we have the actual sequences, we will create the batches, we want to shuffle these sequences into a random order, so the model doesn't overfit to any section of the text, but can instead generate characters given any seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(128, 130), dtype=tf.int32, name=None), TensorSpec(shape=(128, 130), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the shape of dataset is ( (128,130) , (128,130) ) because it is 128 (batch_size) unique sequences of length 130 characters.\n",
    "\n",
    "There are 2 sets because 1st is input sequence and the other one is target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is one hot encoded we are going to use sparse_cross_entropy instead of cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_categorical_crossentropy in module keras.losses:\n",
      "\n",
      "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
      "    Computes the sparse categorical crossentropy loss.\n",
      "    \n",
      "    Standalone usage:\n",
      "    \n",
      "    >>> y_true = [1, 2]\n",
      "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
      "    >>> assert loss.shape == (2,)\n",
      "    >>> loss.numpy()\n",
      "    array([0.0513, 2.303], dtype=float32)\n",
      "    \n",
      "    >>> y_true = [[[ 0,  2],\n",
      "    ...            [-1, -1]],\n",
      "    ...           [[ 0,  2],\n",
      "    ...            [-1, -1]]]\n",
      "    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
      "    ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n",
      "    ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n",
      "    ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n",
      "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
      "    ...   y_true, y_pred, ignore_class=-1)\n",
      "    >>> loss.numpy()\n",
      "    array([[[2.3841855e-07, 2.3841855e-07],\n",
      "            [0.0000000e+00, 0.0000000e+00]],\n",
      "           [[2.3841855e-07, 6.9314730e-01],\n",
      "            [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n",
      "    \n",
      "    Args:\n",
      "      y_true: Ground truth values.\n",
      "      y_pred: The predicted values.\n",
      "      from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "        default, we assume that `y_pred` encodes a probability distribution.\n",
      "      axis: Defaults to -1. The dimension along which the entropy is\n",
      "        computed.\n",
      "      ignore_class: Optional integer. The ID of a class to be ignored during\n",
      "        loss computation. This is useful, for example, in segmentation\n",
      "        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n",
      "        maps. By default (`ignore_class=None`), all classes are considered.\n",
      "    \n",
      "    Returns:\n",
      "      Sparse categorical crossentropy loss value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
    "    Computes the sparse categorical crossentropy loss.\n",
    "\n",
    "**INBUILT FUNCTION**\n",
    "We want logits = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding a big single RNN layer instead we can also do many smaller RNN layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vocab_size`: Size of the vocabulary (total number of unique words or tokens in the text corpus).\n",
    "- `embed_dim`: Dimensionality of word embeddings (length of the dense vector representation of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rnn_neurons`: The number of neurons or units in the GRU layer. It determines the dimensionality of the GRU's output and internal state.\n",
    "- `return_sequences`: A boolean parameter that specifies whether the GRU layer should return the full sequence of outputs (`True`) or just the last output (`False`) when processing a sequence.\n",
    "- `stateful`: If `True`, the states from the previous batch will be used as the initial states for the current batch.\n",
    "- `recurrent_initializer`: The initialization method for the recurrent weights of the GRU. In this case, 'glorot_uniform' is used, which is a variation of the Xavier/Glorot uniform initializer for the recurrent weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU\n",
    "\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # Recurrent_initializer = \"glorot_uniform\" just research, nothing to explain\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (128, None, 64)           5376      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (128, None, 1026)         3361176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (128, None, 84)           86268     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    rnn_neurons=rnn_neurons,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 130, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 130, 84), dtype=float32, numpy=\n",
       "array([[[ 2.32748548e-03, -4.01643943e-03,  9.90648987e-05, ...,\n",
       "          1.39388442e-03,  4.34970949e-03,  4.51245159e-03],\n",
       "        [-5.35512250e-03,  1.44164555e-03, -2.41996255e-04, ...,\n",
       "          4.21193475e-03,  6.97787327e-04,  7.85241649e-03],\n",
       "        [ 3.14112846e-03, -3.39319766e-03,  8.97041522e-03, ...,\n",
       "          4.38333582e-03, -2.63108267e-03,  1.44945942e-02],\n",
       "        ...,\n",
       "        [ 1.48687446e-02,  1.37571304e-03, -1.12667652e-02, ...,\n",
       "         -3.48050380e-05, -1.08929714e-02, -4.19229409e-03],\n",
       "        [ 1.63475554e-02,  2.53165001e-03, -1.10104848e-02, ...,\n",
       "          1.11210172e-03, -1.06050512e-02, -4.70345467e-03],\n",
       "        [ 1.72748789e-02,  3.21802683e-03, -1.04734395e-02, ...,\n",
       "          1.73274032e-03, -1.03259888e-02, -4.90253605e-03]],\n",
       "\n",
       "       [[ 2.32748548e-03, -4.01643943e-03,  9.90648987e-05, ...,\n",
       "          1.39388442e-03,  4.34970949e-03,  4.51245159e-03],\n",
       "        [ 3.75181343e-03, -1.46720130e-02,  2.34887959e-03, ...,\n",
       "         -8.10362166e-04,  1.64661417e-03, -3.47980624e-03],\n",
       "        [ 6.30629109e-03, -9.42074228e-03, -2.04296783e-03, ...,\n",
       "          4.52533830e-03, -2.80487537e-03, -6.19589817e-03],\n",
       "        ...,\n",
       "        [ 1.60834473e-02,  2.92164832e-03, -1.04054250e-02, ...,\n",
       "          1.52093614e-03, -1.05003631e-02, -4.72002383e-03],\n",
       "        [ 9.80788562e-03,  1.82143156e-03, -6.78596320e-03, ...,\n",
       "          7.17307907e-03, -6.22285623e-03,  6.37720618e-03],\n",
       "        [ 1.47157186e-03, -7.72476848e-03, -2.34895432e-03, ...,\n",
       "          6.92901015e-03, -7.85408542e-03, -1.69044686e-03]],\n",
       "\n",
       "       [[-2.36173207e-03,  2.26938725e-03, -2.01137876e-03, ...,\n",
       "          5.58930589e-03,  3.72972660e-04,  4.48853057e-03],\n",
       "        [ 1.09481672e-03, -3.50545091e-03, -1.09967310e-03, ...,\n",
       "          4.46876558e-03,  5.99838234e-03,  6.53572846e-03],\n",
       "        [-1.52819569e-03, -7.72101432e-03,  8.89095943e-04, ...,\n",
       "          3.57312453e-03,  3.86315142e-03,  9.15308483e-03],\n",
       "        ...,\n",
       "        [ 1.36517477e-03, -9.70052369e-03,  1.09040877e-04, ...,\n",
       "          3.06550600e-03, -5.79972146e-03, -6.70654327e-03],\n",
       "        [ 6.72665238e-03, -4.17929888e-03, -2.18946859e-03, ...,\n",
       "         -2.04421091e-03, -1.50809856e-03, -1.11732306e-03],\n",
       "        [ 1.08973291e-02, -1.59808085e-03, -6.27722964e-03, ...,\n",
       "         -1.44921406e-03, -7.84019381e-03, -4.78338171e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.64575456e-03,  1.71088066e-03, -3.01099964e-04, ...,\n",
       "         -4.43374226e-03, -2.13412987e-03, -1.93907786e-03],\n",
       "        [-5.22612594e-03, -5.72732557e-03,  2.87510746e-04, ...,\n",
       "          1.89957907e-04, -5.64741809e-03,  2.06218683e-05],\n",
       "        [ 3.99924489e-03, -5.30041847e-03,  5.63013833e-04, ...,\n",
       "          1.17980340e-03, -6.71183318e-03, -3.07187741e-03],\n",
       "        ...,\n",
       "        [-3.33635858e-03, -1.71574368e-03,  1.56402960e-03, ...,\n",
       "          4.84729279e-03, -1.16054702e-03,  1.05842482e-03],\n",
       "        [-2.84827757e-03, -3.50095844e-03,  9.57172597e-05, ...,\n",
       "          2.58421712e-03, -1.00346550e-03, -2.93818861e-03],\n",
       "        [-4.75816848e-03, -5.54668345e-03,  2.32816976e-03, ...,\n",
       "          1.42563297e-03, -7.69313192e-05,  6.52448321e-03]],\n",
       "\n",
       "       [[-6.79444638e-05, -2.29133829e-03, -6.12126430e-04, ...,\n",
       "         -3.06382659e-04, -1.67877239e-03, -3.38425813e-03],\n",
       "        [ 1.90057582e-03, -1.18289478e-02,  2.82586436e-03, ...,\n",
       "         -2.89659062e-03, -1.64402090e-03, -5.48752677e-03],\n",
       "        [-6.77152071e-04, -5.20193530e-03,  4.32112813e-03, ...,\n",
       "         -2.26973300e-03, -1.45727699e-03, -2.43201014e-03],\n",
       "        ...,\n",
       "        [ 1.31421443e-02, -4.42293705e-04, -7.13489298e-03, ...,\n",
       "         -5.98507933e-04, -1.08308122e-02, -3.11629428e-03],\n",
       "        [ 9.67447832e-03, -6.65304996e-03, -5.78967948e-03, ...,\n",
       "          2.27272743e-03, -1.23879062e-02, -1.16745045e-03],\n",
       "        [ 5.80473687e-04, -1.06121590e-02, -1.39005785e-03, ...,\n",
       "          4.20435518e-03, -9.48191620e-03,  6.98952470e-04]],\n",
       "\n",
       "       [[ 7.92002864e-03,  8.71803670e-04, -5.06036449e-03, ...,\n",
       "          4.36240807e-04, -6.26284489e-03, -3.42450012e-03],\n",
       "        [ 1.21370358e-02,  1.98112847e-03, -7.47566018e-03, ...,\n",
       "          1.01596257e-03, -8.59495066e-03, -4.61198296e-03],\n",
       "        [ 1.46264825e-02,  2.83721182e-03, -8.51015188e-03, ...,\n",
       "          1.49225304e-03, -9.43192653e-03, -4.97621670e-03],\n",
       "        ...,\n",
       "        [-5.29777911e-03,  1.56074809e-03,  9.88830463e-04, ...,\n",
       "          4.61257994e-03, -3.29624093e-03,  4.67081415e-03],\n",
       "        [ 5.54534933e-03,  9.52611794e-04, -4.00125561e-03, ...,\n",
       "          1.59460341e-03, -6.88013714e-03, -5.42777590e-04],\n",
       "        [ 8.18195660e-03,  3.95705365e-03, -2.94504012e-03, ...,\n",
       "          6.12419983e-03,  3.95167433e-03,  1.00032135e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "    # Predict off some random batch\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "    # Display the dimensions of the predictions\n",
    "    print(example_batch_predictions.shape,\n",
    "        \" <=== (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "example_batch_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(130, 1), dtype=int64, numpy=\n",
       "array([[19],\n",
       "       [13],\n",
       "       [76],\n",
       "       [ 7],\n",
       "       [ 4],\n",
       "       [59],\n",
       "       [14],\n",
       "       [65],\n",
       "       [24],\n",
       "       [33],\n",
       "       [30],\n",
       "       [60],\n",
       "       [19],\n",
       "       [38],\n",
       "       [50],\n",
       "       [ 2],\n",
       "       [50],\n",
       "       [54],\n",
       "       [80],\n",
       "       [11],\n",
       "       [77],\n",
       "       [71],\n",
       "       [27],\n",
       "       [ 7],\n",
       "       [40],\n",
       "       [15],\n",
       "       [ 4],\n",
       "       [73],\n",
       "       [58],\n",
       "       [51],\n",
       "       [61],\n",
       "       [26],\n",
       "       [69],\n",
       "       [70],\n",
       "       [ 8],\n",
       "       [73],\n",
       "       [28],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [62],\n",
       "       [ 2],\n",
       "       [ 8],\n",
       "       [42],\n",
       "       [36],\n",
       "       [50],\n",
       "       [38],\n",
       "       [20],\n",
       "       [24],\n",
       "       [26],\n",
       "       [11],\n",
       "       [ 6],\n",
       "       [21],\n",
       "       [48],\n",
       "       [36],\n",
       "       [72],\n",
       "       [65],\n",
       "       [16],\n",
       "       [28],\n",
       "       [70],\n",
       "       [38],\n",
       "       [25],\n",
       "       [ 2],\n",
       "       [49],\n",
       "       [55],\n",
       "       [ 2],\n",
       "       [20],\n",
       "       [79],\n",
       "       [50],\n",
       "       [77],\n",
       "       [10],\n",
       "       [55],\n",
       "       [69],\n",
       "       [33],\n",
       "       [58],\n",
       "       [52],\n",
       "       [27],\n",
       "       [12],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [39],\n",
       "       [58],\n",
       "       [17],\n",
       "       [50],\n",
       "       [63],\n",
       "       [64],\n",
       "       [50],\n",
       "       [61],\n",
       "       [23],\n",
       "       [76],\n",
       "       [29],\n",
       "       [49],\n",
       "       [14],\n",
       "       [59],\n",
       "       [26],\n",
       "       [43],\n",
       "       [41],\n",
       "       [26],\n",
       "       [ 6],\n",
       "       [ 1],\n",
       "       [15],\n",
       "       [77],\n",
       "       [71],\n",
       "       [36],\n",
       "       [76],\n",
       "       [49],\n",
       "       [27],\n",
       "       [ 7],\n",
       "       [25],\n",
       "       [47],\n",
       "       [47],\n",
       "       [19],\n",
       "       [ 9],\n",
       "       [13],\n",
       "       [51],\n",
       "       [58],\n",
       "       [35],\n",
       "       [71],\n",
       "       [ 1],\n",
       "       [26],\n",
       "       [ 5],\n",
       "       [22],\n",
       "       [46],\n",
       "       [73],\n",
       "       [21],\n",
       "       [69],\n",
       "       [53],\n",
       "       [39],\n",
       "       [ 1],\n",
       "       [14],\n",
       "       [44]], dtype=int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 13, 76,  7,  4, 59, 14, 65, 24, 33, 30, 60, 19, 38, 50,  2, 50,\n",
       "       54, 80, 11, 77, 71, 27,  7, 40, 15,  4, 73, 58, 51, 61, 26, 69, 70,\n",
       "        8, 73, 28,  2,  1, 62,  2,  8, 42, 36, 50, 38, 20, 24, 26, 11,  6,\n",
       "       21, 48, 36, 72, 65, 16, 28, 70, 38, 25,  2, 49, 55,  2, 20, 79, 50,\n",
       "       77, 10, 55, 69, 33, 58, 52, 27, 12,  8,  9, 39, 58, 17, 50, 63, 64,\n",
       "       50, 61, 23, 76, 29, 49, 14, 59, 26, 43, 41, 26,  6,  1, 15, 77, 71,\n",
       "       36, 76, 49, 27,  7, 25, 47, 47, 19,  9, 13, 51, 58, 35, 71,  1, 26,\n",
       "        5, 22, 46, 73, 21, 69, 53, 39,  1, 14, 44], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq: \n",
      "\n",
      "old acquaintance tell.  \n",
      "    For thee, against my self I'll vow debate,\n",
      "    For I must ne'er love him whom thou dost hate.\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "Next Char Predictions: \n",
      "\n",
      "82u)&d3j>HEe8MY!Y_y0vpB)O4&rcZfAno,rC! g!,QKYM9>A0(:WKqj5CoM?!X`!9xYv.`nHc[B1,-Nc6YhiYf<uDX3dARPA( 4vpKuXB)?VV8-2ZcJp A';Ur:n]N 3S\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the input seq: \\n\")\n",
    "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
    "print('\\n')\n",
    "print(\"Next Char Predictions: \\n\")\n",
    "print(\"\".join(ind_to_char[sampled_indices ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After confirming the dimensions are working, let's train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "324/324 [==============================] - 93s 274ms/step - loss: 2.6032\n",
      "Epoch 2/30\n",
      "324/324 [==============================] - 93s 274ms/step - loss: 1.8046\n",
      "Epoch 3/30\n",
      "324/324 [==============================] - 87s 260ms/step - loss: 1.5144\n",
      "Epoch 4/30\n",
      "324/324 [==============================] - 85s 254ms/step - loss: 1.3740\n",
      "Epoch 5/30\n",
      "324/324 [==============================] - 84s 253ms/step - loss: 1.2994\n",
      "Epoch 6/30\n",
      "275/324 [========================>.....] - ETA: 12s - loss: 1.2567"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(dataset,epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
